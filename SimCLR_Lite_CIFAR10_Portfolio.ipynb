{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# SimCLR‑Lite (Self‑Supervised Contrastive Learning) on CIFAR‑10  \n**Portfolio notebook (Deep Learning / Representation Learning)**\n\nThis notebook implements a minimal, research‑style **SimCLR** pipeline:\n1. Self‑supervised pretraining (NT‑Xent contrastive loss)\n2. Linear probing (freeze encoder, train a linear classifier)\n3. t‑SNE visualization of learned representations\n\nTip for CPU: reduce epochs, batch size, and number of t‑SNE samples.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0) Install & imports\nIf you're on Colab, you can run the install cell. If you already have PyTorch, skip it.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# (Optional) Install dependencies (useful on Google Colab)\n# !pip -q install torch torchvision tqdm numpy matplotlib scikit-learn\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import os, time, json, random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.models import resnet18\n\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\n\nprint(\"torch:\", torch.__version__)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"device:\", device)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Reproducibility & config\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "SEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\n# Training config (safe defaults)\nEPOCHS_SSL = 10          # increase to 50-200 for stronger results\nBATCH_SIZE = 256         # CPU: try 64 or 128\nLR_SSL = 3e-4\nTEMPERATURE = 0.2\nPROJ_DIM = 128\n\nEPOCHS_LINEAR = 15       # increase to 30-100 for better linear probe\nLR_LINEAR = 0.1\n\nNUM_WORKERS = 2\nPIN_MEMORY = True if device == \"cuda\" else False\n\nOUT_DIR = \"outputs_notebook\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\nprint(\"Config loaded.\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) SimCLR augmentations (two random views)\nSimCLR learns by comparing two different augmentations of the same image.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "class TwoCropsTransform:\n    # Create two random augmented views of the same image\n    def __init__(self, base_transform):\n        self.base_transform = base_transform\n\n    def __call__(self, x):\n        q = self.base_transform(x)\n        k = self.base_transform(x)\n        return q, k\n\ndef simclr_augment(image_size=32):\n    # SimCLR-style augmentations tuned for CIFAR-10 size\n    color_jitter = transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n    return transforms.Compose([\n        transforms.RandomResizedCrop(image_size, scale=(0.2, 1.0)),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomApply([color_jitter], p=0.8),\n        transforms.RandomGrayscale(p=0.2),\n        transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n    ])\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Dataset & dataloader (SSL)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "ssl_transform = TwoCropsTransform(simclr_augment(32))\ntrain_ssl_ds = CIFAR10(root=\"data\", train=True, download=True, transform=ssl_transform)\n\ntrain_ssl_dl = DataLoader(\n    train_ssl_ds,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=NUM_WORKERS,\n    pin_memory=PIN_MEMORY,\n    drop_last=True,\n)\n\nprint(\"SSL train batches:\", len(train_ssl_dl))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Model: ResNet18 encoder + projection head\nWe use a CIFAR-friendly ResNet18 (3×3 conv, no maxpool) + MLP projection head.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "class ProjectionHead(nn.Module):\n    def __init__(self, in_dim, hidden_dim=512, out_dim=128):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, out_dim),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass SimCLRLite(nn.Module):\n    def __init__(self, proj_dim=128):\n        super().__init__()\n        backbone = resnet18(weights=None)\n        backbone.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        backbone.maxpool = nn.Identity()\n        feat_dim = backbone.fc.in_features\n        backbone.fc = nn.Identity()\n\n        self.backbone = backbone\n        self.projector = ProjectionHead(feat_dim, hidden_dim=512, out_dim=proj_dim)\n\n    def encode(self, x):\n        return self.backbone(x)\n\n    def forward(self, x):\n        h = self.encode(x)\n        z = self.projector(h)\n        z = F.normalize(z, dim=1)\n        return h, z\n\nmodel = SimCLRLite(proj_dim=PROJ_DIM).to(device)\nprint(\"Model ready.\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) NT-Xent loss (SimCLR objective)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def nt_xent_loss(z1, z2, temperature=0.2):\n    # z1, z2: [B, D] normalized\n    batch_size = z1.size(0)\n    z = torch.cat([z1, z2], dim=0)                 # [2B, D]\n    sim = torch.mm(z, z.t()) / temperature         # [2B, 2B]\n\n    # mask self similarity\n    mask = torch.eye(2 * batch_size, device=z.device).bool()\n    sim = sim.masked_fill(mask, float(\"-inf\"))\n\n    # positives are diagonal offsets by B\n    pos = torch.cat([torch.diag(sim, batch_size), torch.diag(sim, -batch_size)], dim=0)  # [2B]\n\n    loss = -pos + torch.logsumexp(sim, dim=1)\n    return loss.mean()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) Self-supervised training\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "optimizer = optim.AdamW(model.parameters(), lr=LR_SSL, weight_decay=1e-4)\n\nssl_losses = []\nmodel.train()\n\nstart = time.time()\nfor epoch in range(1, EPOCHS_SSL + 1):\n    running = 0.0\n    pbar = tqdm(train_ssl_dl, desc=f\"SSL Epoch {epoch}/{EPOCHS_SSL}\")\n    for (x1, x2), _ in pbar:\n        x1 = x1.to(device, non_blocking=True)\n        x2 = x2.to(device, non_blocking=True)\n\n        _, z1 = model(x1)\n        _, z2 = model(x2)\n        loss = nt_xent_loss(z1, z2, temperature=TEMPERATURE)\n\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n\n        running += loss.item()\n        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n\n    avg = running / len(train_ssl_dl)\n    ssl_losses.append(avg)\n    print(f\"Epoch {epoch}: avg_ssl_loss={avg:.4f}\")\n\nelapsed = time.time() - start\nprint(f\"SSL training done in {elapsed/60:.1f} min\")\n\nckpt_path = os.path.join(OUT_DIR, \"simclr_lite_cifar10.pt\")\ntorch.save({\"model\": model.state_dict(), \"config\": {\n    \"EPOCHS_SSL\": EPOCHS_SSL,\n    \"BATCH_SIZE\": BATCH_SIZE,\n    \"LR_SSL\": LR_SSL,\n    \"TEMPERATURE\": TEMPERATURE,\n    \"PROJ_DIM\": PROJ_DIM,\n    \"SEED\": SEED,\n}}, ckpt_path)\nprint(\"Saved checkpoint:\", ckpt_path)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Plot SSL loss\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "plt.figure(figsize=(6,4))\nplt.plot(range(1, len(ssl_losses)+1), ssl_losses)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"NT-Xent Loss\")\nplt.title(\"SimCLR SSL Training Loss\")\nplt.grid(True)\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7) Linear probe (freeze encoder)\nWe train a linear classifier on frozen features to measure representation quality.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "tf_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n])\ntf_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n])\n\ntrain_sup_ds = CIFAR10(root=\"data\", train=True, download=True, transform=tf_train)\ntest_sup_ds  = CIFAR10(root=\"data\", train=False, download=True, transform=tf_test)\n\ntrain_sup_dl = DataLoader(train_sup_ds, batch_size=BATCH_SIZE, shuffle=True,\n                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\ntest_sup_dl  = DataLoader(test_sup_ds, batch_size=BATCH_SIZE, shuffle=False,\n                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n\nmodel.eval()\nfor p in model.parameters():\n    p.requires_grad = False\n\nwith torch.no_grad():\n    x, _ = next(iter(train_sup_dl))\n    x = x.to(device)\n    feat_dim = model.encode(x[:2]).shape[1]\n\nprint(\"Feature dim:\", feat_dim)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "class LinearClassifier(nn.Module):\n    def __init__(self, in_dim, num_classes=10):\n        super().__init__()\n        self.fc = nn.Linear(in_dim, num_classes)\n\n    def forward(self, x):\n        return self.fc(x)\n\ndef acc(logits, y):\n    return (logits.argmax(1) == y).float().mean().item()\n\nclf = LinearClassifier(feat_dim, 10).to(device)\ncriterion = nn.CrossEntropyLoss()\nopt = optim.SGD(clf.parameters(), lr=LR_LINEAR, momentum=0.9, weight_decay=1e-4)\nsched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS_LINEAR)\n\ntrain_acc_hist, test_acc_hist, train_loss_hist = [], [], []\n\nfor epoch in range(1, EPOCHS_LINEAR + 1):\n    clf.train()\n    run_loss, run_acc = 0.0, 0.0\n    pbar = tqdm(train_sup_dl, desc=f\"Linear Epoch {epoch}/{EPOCHS_LINEAR}\")\n    for x, y in pbar:\n        x = x.to(device, non_blocking=True)\n        y = y.to(device, non_blocking=True)\n\n        with torch.no_grad():\n            feat = model.encode(x)\n\n        logits = clf(feat)\n        loss = criterion(logits, y)\n\n        opt.zero_grad(set_to_none=True)\n        loss.backward()\n        opt.step()\n\n        a = acc(logits, y)\n        run_loss += loss.item()\n        run_acc += a\n        pbar.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{a*100:.1f}%\")\n\n    sched.step()\n\n    train_loss = run_loss / len(train_sup_dl)\n    train_acc = run_acc / len(train_sup_dl)\n\n    clf.eval()\n    test_acc_sum = 0.0\n    with torch.no_grad():\n        for x, y in test_sup_dl:\n            x = x.to(device, non_blocking=True)\n            y = y.to(device, non_blocking=True)\n            feat = model.encode(x)\n            logits = clf(feat)\n            test_acc_sum += acc(logits, y)\n    test_acc = test_acc_sum / len(test_sup_dl)\n\n    train_loss_hist.append(train_loss)\n    train_acc_hist.append(train_acc)\n    test_acc_hist.append(test_acc)\n\n    print(f\"Epoch {epoch}: train_loss={train_loss:.4f}, train_acc={train_acc*100:.2f}%, test_acc={test_acc*100:.2f}%\")\n\nwith open(os.path.join(OUT_DIR, \"linear_probe_log.json\"), \"w\", encoding=\"utf-8\") as f:\n    json.dump({\n        \"train_loss\": train_loss_hist,\n        \"train_acc\": train_acc_hist,\n        \"test_acc\": test_acc_hist\n    }, f, indent=2)\n\nprint(\"Saved linear probe log:\", os.path.join(OUT_DIR, \"linear_probe_log.json\"))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Plot Linear Probe accuracy\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "plt.figure(figsize=(6,4))\nplt.plot(range(1, len(train_acc_hist)+1), [a*100 for a in train_acc_hist], label=\"Train Acc\")\nplt.plot(range(1, len(test_acc_hist)+1), [a*100 for a in test_acc_hist], label=\"Test Acc\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy (%)\")\nplt.title(\"Linear Probe Accuracy\")\nplt.grid(True)\nplt.legend()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8) t-SNE visualization\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "NUM_SAMPLES_TSNE = 2000  # CPU: 500-1500 | GPU: 5000-10000\n\ntf_plain = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n])\n\ntsne_ds = CIFAR10(root=\"data\", train=False, download=True, transform=tf_plain)\ntsne_dl = DataLoader(tsne_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\nfeatures, labels = [], []\ncollected = 0\n\nmodel.eval()\nwith torch.no_grad():\n    for x, y in tqdm(tsne_dl, desc=\"Collecting features\"):\n        x = x.to(device)\n        h = model.encode(x).detach().cpu().numpy()\n        features.append(h)\n        labels.append(y.numpy())\n        collected += x.size(0)\n        if collected >= NUM_SAMPLES_TSNE:\n            break\n\nX = np.concatenate(features, axis=0)[:NUM_SAMPLES_TSNE]\ny = np.concatenate(labels, axis=0)[:NUM_SAMPLES_TSNE]\nprint(\"X shape:\", X.shape)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "tsne = TSNE(n_components=2, init=\"pca\", learning_rate=\"auto\", perplexity=30, random_state=SEED)\nZ = tsne.fit_transform(X)\n\nplt.figure(figsize=(8,6))\nplt.scatter(Z[:,0], Z[:,1], c=y, s=6, alpha=0.8)\nplt.title(\"CIFAR-10 Representations (t-SNE)\")\nplt.xlabel(\"Dim-1\")\nplt.ylabel(\"Dim-2\")\nplt.grid(True)\nplt.show()\n\ntsne_path = os.path.join(OUT_DIR, \"tsne.png\")\nplt.figure(figsize=(8,6))\nplt.scatter(Z[:,0], Z[:,1], c=y, s=6, alpha=0.8)\nplt.title(\"CIFAR-10 Representations (t-SNE)\")\nplt.xlabel(\"Dim-1\")\nplt.ylabel(\"Dim-2\")\nplt.grid(True)\nplt.savefig(tsne_path, dpi=200, bbox_inches=\"tight\")\nprint(\"Saved:\", tsne_path)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9) GitHub README suggestions\n- Explain NT-Xent and the role of augmentations  \n- Report final linear probe test accuracy  \n- Add `outputs_notebook/tsne.png` as a figure\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}